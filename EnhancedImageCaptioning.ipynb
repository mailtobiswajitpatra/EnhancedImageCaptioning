{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW8Egko4GQqr"
      },
      "outputs": [],
      "source": [
        "#IMPORT LIBRARIES#\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras_preprocessing.image import load_img\n",
        "import string\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import activations\n",
        "from keras import Input\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from gtts import gTTS\n",
        "#from playsound import playsound\n",
        "from IPython import display\n",
        "import collections\n",
        "import wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import statistics\n",
        "from math import sqrt\n",
        "import imageio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import glob\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import vit_keras\n",
        "from vit_keras import vit\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from vit_keras import vit\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EIh6VTBBhCd"
      },
      "outputs": [],
      "source": [
        "# RETRIEVE TRAINING IMAGES AND ASSOCIATED CAPTIONS\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Path to the directory containing all the images\n",
        "image_directory = 'Path of Image directory'\n",
        "\n",
        "# Path to the file containing all the captions\n",
        "caption_file = 'Path of caption file'\n",
        "\n",
        "# Path to the text file containing the training image IDs\n",
        "training_ids_file = 'Path to retrieve training image id'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "output_folder = 'all_data'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Path to the output image folder\n",
        "output_image_folder = os.path.join(output_folder, 'training_images')\n",
        "os.makedirs(output_image_folder, exist_ok=True)\n",
        "\n",
        "# Path to the output text file\n",
        "output_text_file = os.path.join(output_folder, 'training_captions.txt')\n",
        "\n",
        "# Read the training image IDs from the text file\n",
        "with open(training_ids_file, 'r') as f:\n",
        "    training_ids = f.read().splitlines()\n",
        "\n",
        "\n",
        "TRAIN_LIMIT = \"Number of images using for training\"\n",
        "\n",
        "training_ids = training_ids[:TRAIN_LIMIT] #for limit of train ids\n",
        "\n",
        "\n",
        "# Create a dictionary to store the captions for training images\n",
        "captions = {}\n",
        "\n",
        "# Read the captions from the caption file\n",
        "with open(caption_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            image_id, caption = line.split(',', 1)\n",
        "            image_id = image_id.split('.')[0]  # Remove the file extension for comparison\n",
        "            if image_id in training_ids:\n",
        "                if image_id not in captions:\n",
        "                    captions[image_id] = []\n",
        "                captions[image_id].append(caption.strip())\n",
        "\n",
        "# Write the image IDs and captions to the output text file\n",
        "with open(output_text_file, 'w') as f:\n",
        "    for image_id in training_ids:\n",
        "        # Form the image filename\n",
        "        image_filename = f'{image_id}.jpg'\n",
        "\n",
        "        # Copy the image to the output image folder\n",
        "        src_path = os.path.join(image_directory, image_filename)\n",
        "        dst_path = os.path.join(output_image_folder, image_filename)\n",
        "        if os.path.exists(src_path):\n",
        "            shutil.copyfile(src_path, dst_path)\n",
        "        else:\n",
        "            print(f\"Warning: {image_filename} does not exist in {image_directory}\")\n",
        "\n",
        "        # Write the image ID and captions to the output text file\n",
        "        if image_id in captions:\n",
        "            caption_list = captions[image_id]\n",
        "            for caption in caption_list:\n",
        "                f.write(f\"{image_filename},{caption}\\n\")\n",
        "        else:\n",
        "            f.write(f\"{image_filename},\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqVzpwNGCjp0"
      },
      "outputs": [],
      "source": [
        "# RETRIEVE VALIDATION IMAGES AND ASSOCIATED CAPTIONS\n",
        "\n",
        "# Path to the directory containing all the images\n",
        "image_directory = 'Path of Image directory'\n",
        "\n",
        "# Path to the file containing all the captions\n",
        "caption_file = 'Path of caption file'\n",
        "\n",
        "# Path to the text file containing the training image IDs\n",
        "val_ids_file = 'Path to retrieve validation image id'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "output_folder = 'all_data'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Path to the output image folder\n",
        "output_image_folder = os.path.join(output_folder, 'validation_images')\n",
        "os.makedirs(output_image_folder, exist_ok=True)\n",
        "\n",
        "# Path to the output text file\n",
        "output_text_file = os.path.join(output_folder, 'validation_captions.txt')\n",
        "\n",
        "# Read the training image IDs from the text file\n",
        "with open(val_ids_file, 'r') as f:\n",
        "    valid_ids = f.read().splitlines()\n",
        "\n",
        "VALID_LIMIT = \"Number of images using for validation\"\n",
        "\n",
        "valid_ids = valid_ids[:VALID_LIMIT] #for limit of validation ids\n",
        "\n",
        "\n",
        "# Create a dictionary to store the captions for training images\n",
        "captions = {}\n",
        "\n",
        "# Read the captions from the caption file\n",
        "with open(caption_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            image_id, caption = line.split(',', 1)\n",
        "            image_id = image_id.split('.')[0]  # Remove the file extension for comparison\n",
        "            if image_id in training_ids:\n",
        "                if image_id not in captions:\n",
        "                    captions[image_id] = []\n",
        "                captions[image_id].append(caption.strip())\n",
        "\n",
        "# Write the image IDs and captions to the output text file\n",
        "with open(output_text_file, 'w') as f:\n",
        "    for image_id in training_ids:\n",
        "        # Form the image filename\n",
        "        image_filename = f'{image_id}.jpg'\n",
        "\n",
        "        # Copy the image to the output image folder\n",
        "        src_path = os.path.join(image_directory, image_filename)\n",
        "        dst_path = os.path.join(output_image_folder, image_filename)\n",
        "        if os.path.exists(src_path):\n",
        "            shutil.copyfile(src_path, dst_path)\n",
        "        else:\n",
        "            print(f\"Warning: {image_filename} does not exist in {image_directory}\")\n",
        "\n",
        "        # Write the image ID and captions to the output text file\n",
        "        if image_id in captions:\n",
        "            caption_list = captions[image_id]\n",
        "            for caption in caption_list:\n",
        "                f.write(f\"{image_filename},{caption}\\n\")\n",
        "        else:\n",
        "            f.write(f\"{image_filename},\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luDFTfKYv8hq"
      },
      "outputs": [],
      "source": [
        "# RETRIEVE TESTING IMAGES\n",
        "\n",
        "# Path to the directory containing all the images\n",
        "image_directory = 'Path of Image directory'\n",
        "\n",
        "# Path to the file containing all the captions\n",
        "caption_file = 'Path of caption file'\n",
        "\n",
        "# Path to the text file containing the training image IDs\n",
        "test_ids_file = 'Path to retrieve testing image id'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "output_folder = 'all_data'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Path to the output image folder\n",
        "output_image_folder = os.path.join(output_folder, 'testing_images')\n",
        "os.makedirs(output_image_folder, exist_ok=True)\n",
        "\n",
        "# Path to the output text file\n",
        "output_text_file = os.path.join(output_folder, 'testing_captions.txt')\n",
        "\n",
        "# Read the training image IDs from the text file\n",
        "with open(test_ids_file, 'r') as f:\n",
        "    test_ids = f.read().splitlines()\n",
        "\n",
        "TEST_LIMIT= \"Number of images using for testing\"\n",
        "\n",
        "test_ids = test_ids[:TEST_LIMIT] #for limit of Test ids\n",
        "\n",
        "\n",
        "# Create a dictionary to store the captions for training images\n",
        "captions = {}\n",
        "\n",
        "# Read the captions from the caption file\n",
        "with open(caption_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            image_id, caption = line.split(',', 1)\n",
        "            image_id = image_id.split('.')[0]  # Remove the file extension for comparison\n",
        "            if image_id in training_ids:\n",
        "                if image_id not in captions:\n",
        "                    captions[image_id] = []\n",
        "                captions[image_id].append(caption.strip())\n",
        "\n",
        "# Write the image IDs and captions to the output text file\n",
        "with open(output_text_file, 'w') as f:\n",
        "    for image_id in training_ids:\n",
        "        # Form the image filename\n",
        "        image_filename = f'{image_id}.jpg'\n",
        "\n",
        "        # Copy the image to the output image folder\n",
        "        src_path = os.path.join(image_directory, image_filename)\n",
        "        dst_path = os.path.join(output_image_folder, image_filename)\n",
        "        if os.path.exists(src_path):\n",
        "            shutil.copyfile(src_path, dst_path)\n",
        "        else:\n",
        "            print(f\"Warning: {image_filename} does not exist in {image_directory}\")\n",
        "\n",
        "        # Write the image ID and captions to the output text file\n",
        "        if image_id in captions:\n",
        "            caption_list = captions[image_id]\n",
        "            for caption in caption_list:\n",
        "                f.write(f\"{image_filename},{caption}\\n\")\n",
        "        else:\n",
        "            f.write(f\"{image_filename},\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ0TsrdKwIWf"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "img_patht = 'Path of \"training_images\" directory\n",
        "caption_file = 'Path of \"training_captions.txt\" file'\n",
        "\n",
        "# Initialize lists\n",
        "all_idst = []\n",
        "all_img_vectort = []\n",
        "annotationst = []\n",
        "\n",
        "# Read the captions from the caption file\n",
        "with open(caption_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            image_idt, captiont = line.split(',', 1)\n",
        "            full_flickr_image_patht = os.path.join(img_patht, image_idt)\n",
        "\n",
        "            all_idst.append(image_idt)\n",
        "            all_img_vectort.append(full_flickr_image_patht)\n",
        "            annotationst.append(captiont)\n",
        "\n",
        "# Shuffling the captions and image names together\n",
        "img_idst, all_img_vectort, annotationst = shuffle(all_idst, all_img_vectort, annotationst)\n",
        "\n",
        "num_examples = len(all_idst)\n",
        "\n",
        "img_idst = img_idst[:num_examples]\n",
        "all_img_vectort = all_img_vectort[:num_examples]\n",
        "annotationst = annotationst[:num_examples]\n",
        "\n",
        "# Creating a DataFrame\n",
        "dftrain = pd.DataFrame(list(zip(img_idst, all_img_vectort, annotationst)), columns=['ID', 'Path', 'Caption'])\n",
        "dftrain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plQmtDOKwq-l"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "img_pathv = 'Path of \"validation_images\" directory'\n",
        "caption_file = 'Path of \"validation_captions.txt\" file'\n",
        "\n",
        "# Initialize lists\n",
        "all_idsv = []\n",
        "all_img_vectorv = []\n",
        "annotationsv = []\n",
        "\n",
        "# Read the captions from the caption file\n",
        "with open(caption_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            image_idv, captionv = line.split(',', 1)\n",
        "            full_flickr_image_pathv = os.path.join(img_pathv, image_idv)\n",
        "\n",
        "            all_idsv.append(image_idv)\n",
        "            all_img_vectorv.append(full_flickr_image_pathv)\n",
        "            annotationsv.append(captionv)\n",
        "\n",
        "# Shuffling the captions and image names together\n",
        "img_idsv, all_img_vectorv, annotationsv = shuffle(all_idsv, all_img_vectorv, annotationsv)\n",
        "\n",
        "num_examples = len(all_idsv)\n",
        "\n",
        "img_idsv = img_idsv[:num_examples]\n",
        "all_img_vectorv = all_img_vectorv[:num_examples]\n",
        "annotationsv = annotationsv[:num_examples]\n",
        "\n",
        "# Creating a DataFrame\n",
        "dfval = pd.DataFrame(list(zip(img_idsv, all_img_vectorv, annotationsv)), columns=['ID', 'Path', 'Caption'])\n",
        "dfval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnwEeHSnznXa"
      },
      "outputs": [],
      "source": [
        "#total train and val captions in dataset\n",
        "annotationstotal=annotationst+annotationsv\n",
        "#Create vocabulary & counter for the captions\n",
        "vocabulary = [word.lower() for line in annotationstotal for word in line.split()]\n",
        "val_count = Counter(vocabulary)\n",
        "print(val_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsHSPL5t9bhJ"
      },
      "outputs": [],
      "source": [
        "#Dataframe to contain training and validation images and associated captions\n",
        "img_ids=img_idst+img_idsv\n",
        "all_img_vector=all_img_vectort+all_img_vectorv\n",
        "annotations=annotationst+annotationsv\n",
        "df = pd.DataFrame(list(zip(img_ids,all_img_vector,annotations)),columns =['ID','Path', 'Caption'])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5qv0qdQ3I55"
      },
      "outputs": [],
      "source": [
        "#Visualise the top 20 occuring words in the captions\n",
        "print('No of total words :',len(val_count))\n",
        "for word, count in val_count.most_common(20):\n",
        "  print(word, \": \", count)\n",
        "lst = val_count.most_common(20)\n",
        "most_common_words_df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
        "most_common_words_df.plot.bar(x='Word', y='Count', width=0.6, color='skyblue', figsize=(15, 10))\n",
        "plt.title(\"Top 20 most frequent words before processing\", fontsize = 18, color= 'navy')\n",
        "plt.xlabel(\"Words\", fontsize = 14, color= 'navy')\n",
        "plt.ylabel(\"Count\", fontsize = 14, color= 'navy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQFv6FxK3I5-"
      },
      "outputs": [],
      "source": [
        "#data cleaning\n",
        "rem_punct = str.maketrans('', '', string.punctuation)\n",
        "for r in range(len(annotationstotal)) :\n",
        "  line = annotationstotal[r]\n",
        "  line = line.split()\n",
        "\n",
        "  # remove ambiguity\n",
        "  for word in line:\n",
        "    if (word!=word.lower()):\n",
        "      line = [word.lower() for word in line]\n",
        "\n",
        "  # remove punctuation from each caption and hanging letters\n",
        "  line = [word.translate(rem_punct) for word in line]\n",
        "  line = [word for word in line if len(word) > 0]\n",
        "\n",
        "  # remove numeric values\n",
        "  line = [word for word in line if word.isalpha()]\n",
        "\n",
        "  annotationstotal[r] = ' '.join(line)\n",
        "\n",
        "#add the <start> & <end> token to all those captions as well\n",
        "annotations = ['<start>' + ' ' + line + ' ' + '<end>' for line in annotationstotal]\n",
        "\n",
        "#Create a list which contains all the path to the images\n",
        "all_img_path = all_img_vector\n",
        "##list contatining captions for an image\n",
        "annotations[0:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URNs4LUBNs0J"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "res=0\n",
        "word_counts ={}\n",
        "threshold = 5\n",
        "for line in annotationstotal:\n",
        "  for word in line.split():\n",
        "    word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "print(word_counts)\n",
        "\n",
        "for i in range(len(annotationstotal)):\n",
        "  #print(type(len(annotations[i].split())))\n",
        "  res = res + len(annotationstotal[i].split())\n",
        "res=res-len(annotationstotal)*2\n",
        "#print (\"The number of words in string are : \" + str(res))\n",
        "vocabulary = [word for line in annotationstotal for word in line.split() if word_counts[word] >= threshold]\n",
        "val_count = Counter(vocabulary)\n",
        "unq = len(val_count)-2\n",
        "print(\"No of total unique words in vocabulary after data cleaning excluding <start> and <end>:\",unq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A13VaDq3I6E"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "top_word_cnt = \"Size of informative vocabulary\"\n",
        "tokenizer = Tokenizer(num_words = top_word_cnt+1, filters= '!\"#$%^&*()_+.,:;-?/~`{}[]|\\=@ ',\n",
        "                      lower = True, char_level = False,\n",
        "                      oov_token = 'UNK')\n",
        "\n",
        "# Creating word-to-index and index-to-word mappings.\n",
        "tokenizer.fit_on_texts(annotations)\n",
        "#transform each text into a sequence of integers\n",
        "train_seqs = tokenizer.texts_to_sequences(annotations)\n",
        "\n",
        "# Add PAD token for zero\n",
        "tokenizer.word_index['PAD'] = 0\n",
        "tokenizer.index_word[0] = 'PAD'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj4oVSp-3I6K"
      },
      "outputs": [],
      "source": [
        "# Creating a word count for our tokenizer to visualize the Top 20 occuring words after text processing\n",
        "\n",
        "tokenizer_top_words = [word for line in annotationstotal for word in line.split() ]\n",
        "\n",
        "#tokenizer_top_words_count\n",
        "tokenizer_top_words_count = collections.Counter(tokenizer_top_words)\n",
        "\n",
        "for word, count in tokenizer_top_words_count.most_common(20) :\n",
        "  print(word, \": \", count)\n",
        "\n",
        "tokens = tokenizer_top_words_count.most_common(20)\n",
        "most_com_words_df = pd.DataFrame(tokens, columns = ['Word', 'Count'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DckU33rq-viW"
      },
      "outputs": [],
      "source": [
        "#plot 20 most common occuring words after processing\n",
        "\n",
        "lst2=tokenizer_top_words_count.most_common(20)\n",
        "most_common_words_df=pd.DataFrame(lst2, columns = ['Word','Count'])\n",
        "most_common_words_df.plot.bar(x = 'Word', y= 'Count', width=0.6, color = 'skyblue', figsize = (15, 10))\n",
        "plt.title('Top 20 most frequent words after processing', fontsize =18, color= 'navy')\n",
        "plt.xlabel('Words', fontsize =14, color= 'navy')\n",
        "plt.ylabel('Counts', fontsize =14, color= 'navy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6jFbvVO3I6N"
      },
      "outputs": [],
      "source": [
        "# Pad each vector to the max_length of the captions  store it to a vairable\n",
        "train_seqs_len = [len(seq) for seq in train_seqs]\n",
        "longest_sentence_length = 40 # Maximum length of sentence is taken as 40\n",
        "print(longest_sentence_length)\n",
        "cap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_sentence_length,\n",
        "                                                          dtype='int32', value=0)\n",
        "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))\n",
        "\n",
        "# creating list to store preprocessed images and setting up the Image shape\n",
        "preprocessed_image = []\n",
        "IMAGE_SHAPE = (256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWP_bcfo3I7M"
      },
      "outputs": [],
      "source": [
        "def load_images(image_path) :\n",
        "  img = tf.io.read_file(image_path, name = None)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.resize(img, IMAGE_SHAPE)\n",
        "  img = vit.preprocess_inputs(img)\n",
        "  return img, image_path\n",
        "\n",
        "# Map each image full path to the function, to preprocess the image\n",
        "training_list = sorted(set(all_img_vector))\n",
        "New_Img = tf.data.Dataset.from_tensor_slices(training_list)\n",
        "New_Img = New_Img.map(load_images, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "New_Img = New_Img.batch(32, drop_remainder=False)\n",
        "\n",
        "traincaptionsize = 'Numbers of captions in training'\n",
        "\n",
        "path_train,  caption_train = all_img_vectort, cap_vector[:traincaptionsize]   # Upto traincaptionsize = Numbers of captions in training\n",
        "path_val, caption_val = all_img_vectorv, cap_vector[traincaptionsize:]        # After training captions and Upto validation caption size = Numbers of captions in validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5lEY-ACt6rQ"
      },
      "outputs": [],
      "source": [
        "print(\"Training data for images: \" + str(len(path_train)))\n",
        "print(\"Validation data for images: \" + str(len(path_val)))\n",
        "print(\"Training data for Captions: \" + str(len(caption_train)))\n",
        "print(\"Validation data for Captions: \" + str(len(caption_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWHpskHoevHT"
      },
      "outputs": [],
      "source": [
        "# Feature extraction using Vision Transformer #\n",
        "\n",
        "vit_model = vit.vit_b16(\n",
        "        image_size = IMAGE_SHAPE,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        weights = 'imagenet21k'\n",
        "        )\n",
        "\n",
        "new_input =vit_model.input\n",
        "hidden_layer = vit_model.layers[-2].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "for layer in image_features_extract_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "img_features = {}\n",
        "for image,image_path in tqdm(New_Img):\n",
        "  batch_features = image_features_extract_model(image)\n",
        "  #print(\"Batch Features Shape:\", batch_features.shape)\n",
        "  #batch_features_flattened = tf.reshape(batch_features, (batch_features.shape[0], -1))\n",
        "  for batch_feat, path in zip(batch_features, image_path) :\n",
        "    feature_path = path.numpy().decode('utf-8')\n",
        "    img_features[feature_path] = batch_feat.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUnmdvG1e5TT"
      },
      "outputs": [],
      "source": [
        "image_features_extract_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl4O77Fz3I7W"
      },
      "outputs": [],
      "source": [
        "#func to provide, both images along with the captions as input\n",
        "def map(image_name, caption):\n",
        "    img_tensor = img_features[image_name.decode('utf-8')]\n",
        "    return img_tensor, caption\n",
        "\n",
        "#SETTING BATCH SIZE, BUFFER SIZE\n",
        "#func to transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 32\n",
        "def gen_dataset(img, capt):\n",
        "\n",
        "    data = tf.data.Dataset.from_tensor_slices((img, capt))\n",
        "    data = data.map(lambda ele1, ele2 : tf.numpy_function(map, [ele1, ele2], [tf.float32, tf.int32]),\n",
        "                    num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "    data = (data.shuffle(BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE, drop_remainder = False)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "    return data\n",
        "\n",
        "train_dataset = gen_dataset(path_train,caption_train)\n",
        "val_dataset = gen_dataset(path_val,caption_val)\n",
        "\n",
        "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
        "print(sample_img_batch.shape)\n",
        "print(sample_cap_batch.shape)\n",
        "\n",
        "\n",
        "#Setting  parameters\n",
        "embedding_dim = 512\n",
        "units = 512\n",
        "\n",
        "vocab_size = top_word_cnt+1\n",
        "train_num_steps = len(path_train) // BATCH_SIZE\n",
        "val_num_steps = len(path_val) // BATCH_SIZE\n",
        "\n",
        "max_length = longest_sentence_length\n",
        "feature_shape = batch_feat.shape[1]\n",
        "attention_feature_shape = batch_feat.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQIUFJsM3I7a"
      },
      "outputs": [],
      "source": [
        "# Creating Encoder, Decoder and Attention Module\n",
        "\n",
        "class Encoder(Model):\n",
        "    def __init__(self,embed_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "    def call(self, features):\n",
        "        features =  self.dense(features) # extract the features from the image shape\n",
        "        features =  tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0)\n",
        "        return features\n",
        "\n",
        "class Attention(Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.w1 = tf.keras.layers.Dense(units)\n",
        "        self.w2 = tf.keras.layers.Dense(units)\n",
        "        self.w3 = tf.keras.layers.Dense(1)\n",
        "        self.units=units\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = hidden[:, tf.newaxis]\n",
        "        score = tf.keras.activations.tanh(self.w1(features) + self.w2(hidden_with_time_axis))  # encoder features, decoder hidden state\n",
        "        attention_weights = tf.keras.activations.softmax(self.w3(score), axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class Decoder(Model):\n",
        "    def __init__(self, embed_dim, units, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.units=units\n",
        "        self.attention = Attention(self.units) #iniitalize Attention model by units\n",
        "        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #building Embedding layer\n",
        "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "        self.d1 = tf.keras.layers.Dense(self.units) #build Dense layer\n",
        "        self.d2 = tf.keras.layers.Dense(vocab_size) #build Dense layer\n",
        "\n",
        "\n",
        "    def call(self,x,features, hidden):\n",
        "        context_vector, attention_weights = self.attention(features, hidden)     #create your context vector & attention weights from attention model\n",
        "        embed = self.embed(x)                                                    # embed your input to shape\n",
        "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer\n",
        "        output,state = self.gru(embed)                                           # Extract the output & hidden state from GRU layer.\n",
        "        output = self.d1(output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        output = self.d2(output)\n",
        "        return output, state, attention_weights\n",
        "\n",
        "    def init_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OOSAGoO3I7g"
      },
      "outputs": [],
      "source": [
        "encoder=Encoder(embedding_dim)\n",
        "decoder=Decoder(embedding_dim, units, vocab_size)\n",
        "features=encoder(sample_img_batch)\n",
        "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
        "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
        "\n",
        "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
        "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
        "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch_size,vocab_size)\n",
        "print('Attention weights shape: {}'.format(attention_weights.shape)) #(batch_size,257, embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGMq9BCl3I7h"
      },
      "outputs": [],
      "source": [
        "#defining the optimizer\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate = 0.001)\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "#optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001)\n",
        "\n",
        "#LOSS FUNCTION#\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE) #define your loss object\n",
        "#loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "checkpoint_path = \"/content/checkpoint\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        encoder_op = encoder(img_tensor)\n",
        "        for r in range(1, target.shape[1]) :\n",
        "          predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
        "          loss = loss + loss_function(target[:, r], predictions)\n",
        "          dec_input = tf.expand_dims(target[:, r], 1)\n",
        "\n",
        "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    grad = tape.gradient (loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(grad, trainable_vars))\n",
        "\n",
        "    return loss, avg_loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    all_predictions = []\n",
        "    hidden = decoder.init_state(batch_size = target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_op = encoder(img_tensor)\n",
        "      for r in range(1, target.shape[1]) :\n",
        "        predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
        "        all_predictions.append(predictions)\n",
        "        loss = loss + loss_function(target[:, r], predictions)\n",
        "        dec_input = tf.expand_dims(target[: , r], 1)\n",
        "\n",
        "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    grad = tape.gradient (loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(grad, trainable_vars))\n",
        "    all_predictions = tf.stack(all_predictions, axis=1)\n",
        "    return loss, avg_loss, predictions, all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbHEGSj6h8jg"
      },
      "outputs": [],
      "source": [
        "# EVALUATION METRICS #\n",
        "!pip install pycocoevalcap\n",
        "from collections import Counter, defaultdict\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "import contextlib\n",
        "import io\n",
        "import hashlib\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "f = io.StringIO()\n",
        "\n",
        "bleu = Bleu(n=4)\n",
        "meteor = Meteor()\n",
        "rouge = Rouge()\n",
        "cider = Cider()\n",
        "spice = Spice()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsUFOyKdh9kG"
      },
      "outputs": [],
      "source": [
        "def decode_caption(caption):\n",
        "    decoded_caption = []\n",
        "    # print(type(caption))\n",
        "    for word_id in caption:\n",
        "        if word_id == 0:  # Padding token, ignore\n",
        "            continue\n",
        "        if word_id == tokenizer.word_index['<end>']:  # Stop decoding at <end> token\n",
        "            break\n",
        "        if word_id == tokenizer.word_index['<start>']:  # Ignore <start> token\n",
        "            continue\n",
        "        decoded_caption.append(tokenizer.index_word[word_id])\n",
        "    return decoded_caption\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwFehjt6DaAn"
      },
      "outputs": [],
      "source": [
        "def hash_image_tensor(img_tensor):\n",
        "    \"\"\"Hash image tensor to create a unique identifier for each image.\"\"\"\n",
        "    return hashlib.sha256(img_tensor.numpy().tobytes()).hexdigest()\n",
        "\n",
        "def list_to_dict(input_list):\n",
        "  return {i+1: [item] for i, item in enumerate(input_list)}\n",
        "\n",
        "def take_first_three_elements(input_dict):\n",
        "    return {key: value[:3] for key, value in input_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS-z3q-5iHN0"
      },
      "outputs": [],
      "source": [
        "# MODEL TRAINING #\n",
        "\n",
        "train_loss_plot = []\n",
        "val_loss_plot = []\n",
        "test_loss_plot = []\n",
        "best_val_loss=100\n",
        "best_test_loss=100\n",
        "bleu1_scores = []\n",
        "bleu2_scores = []\n",
        "bleu3_scores = []\n",
        "bleu4_scores = []\n",
        "best_scores = {\n",
        "    'BLEU-1': 0.0,\n",
        "    'BLEU-2': 0.0,\n",
        "    'BLEU-3': 0.0,\n",
        "    'BLEU-4': 0.0,\n",
        "    'METEOR': 0.0,\n",
        "    'ROUGE-1': 0.0,\n",
        "    'ROUGE-2': 0.0,\n",
        "    'ROUGE-L': 0.0,\n",
        "    'CIDEr' : 0.0,\n",
        "    'SPICE' :0.0\n",
        "\n",
        "}\n",
        "bleu1_scores_plot = []\n",
        "bleu2_scores_plot = []\n",
        "bleu3_scores_plot = []\n",
        "bleu4_scores_plot = []\n",
        "meteor_scores_plot = []\n",
        "rouge1_scores_plot = []\n",
        "rouge2_scores_plot = []\n",
        "rouge_l_scores_plot = []\n",
        "cider_scores_plot = []\n",
        "spice_scores_plot = []\n",
        "\n",
        "patience = 2  # For repetition 3 times #\n",
        "wait = 0\n",
        "early_stop = False\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in tqdm(range(1, EPOCHS+1)):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    total_loss_val=0\n",
        "    image_ids = []\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    references_dict = {}\n",
        "    hypotheses_dict = {}\n",
        "\n",
        "    for (batch, (img_tensor, tar)) in enumerate(train_dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, tar)\n",
        "        total_loss += t_loss\n",
        "        avg_train_loss = total_loss / train_num_steps\n",
        "\n",
        "    for (batch, (img_tensor, tar)) in enumerate(val_dataset) :\n",
        "        tar_real = tar[:, 1:]\n",
        "        batch_loss, loss, predictions, all_predictions = val_step(img_tensor, tar)\n",
        "        total_loss_val = total_loss_val + loss\n",
        "        avg_val_loss = total_loss_val / val_num_steps\n",
        "\n",
        "        for j in range(tar_real.shape[0]):\n",
        "            img_id =  hash_image_tensor(img_tensor[j])\n",
        "            reference = decode_caption(tf.boolean_mask(tar_real[j], tar_real[j] != 0).numpy())\n",
        "            hypothesis = decode_caption(tf.argmax(all_predictions[j], axis = -1).numpy())\n",
        "            references.append(\" \".join(reference))  # Join the list of words into a single string\n",
        "            hypotheses.append(\" \".join(hypothesis))  # Join the list of words into a single string\n",
        "\n",
        "            if img_id not in references_dict:\n",
        "                references_dict[img_id] = []\n",
        "            references_dict[img_id].append(\" \".join(reference))\n",
        "            if img_id not in hypotheses_dict:\n",
        "                hypotheses_dict[img_id] = []\n",
        "            hypotheses_dict[img_id].append(\" \".join(hypothesis))\n",
        "\n",
        "    references = list_to_dict(references)\n",
        "    hypotheses = list_to_dict(hypotheses)\n",
        "\n",
        "    unique_references = list(references_dict.values())\n",
        "    unique_hypotheses = list(hypotheses_dict.values())\n",
        "\n",
        "    prep_references = {}\n",
        "    prep_hypotheses = {}\n",
        "    new_ref = {}\n",
        "    num_images = len(unique_references)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img_id = str(i + 1)\n",
        "        prep_references[img_id] = unique_references[i]########  prepare ref and hyp for multiple ref and single hyp\n",
        "        prep_hypotheses[img_id] = unique_hypotheses[i][-1]\n",
        "        # prep_hypotheses[img_id] = [unique_hypotheses[i][-1]]\n",
        "\n",
        "    prep_hypotheses = {image_id: [sentence] for image_id, sentence in prep_hypotheses.items()}\n",
        "    # new_ref = take_first_three_elements(prep_references)\n",
        "    with contextlib.redirect_stdout(f):\n",
        "        bleu_scores, _ = bleu.compute_score(prep_references, prep_hypotheses)\n",
        "\n",
        "    rouge_score, _ = rouge.compute_score(prep_references, prep_hypotheses)\n",
        "\n",
        "    meteor_score, _ = meteor.compute_score(prep_references, prep_hypotheses)\n",
        "\n",
        "    cider_score, _ = cider.compute_score(prep_references, prep_hypotheses)\n",
        "\n",
        "    spice_score, _ = spice.compute_score(prep_references, prep_hypotheses)\n",
        "\n",
        "    train_loss_plot.append(avg_train_loss)\n",
        "    val_loss_plot.append(avg_val_loss)\n",
        "\n",
        "    # FOR SCORE OF DIFFERENT METRICS #\n",
        "    best_scores['BLEU-1'] = max(best_scores['BLEU-1'], bleu_scores[0])\n",
        "    best_scores['BLEU-2'] = max(best_scores['BLEU-2'], bleu_scores[1])\n",
        "    best_scores['BLEU-3'] = max(best_scores['BLEU-3'], bleu_scores[2])\n",
        "    best_scores['BLEU-4'] = max(best_scores['BLEU-4'], bleu_scores[3])\n",
        "    best_scores['METEOR'] = max(best_scores['METEOR'], meteor_score)\n",
        "    best_scores['ROUGE-L'] = max(best_scores['ROUGE-L'], rouge_score)\n",
        "    best_scores['CIDEr'] = max(best_scores['CIDEr'], cider_score)\n",
        "    best_scores['SPICE'] = max(best_scores['SPICE'], spice_score)\n",
        "\n",
        "    # FOR PLOTTING SCORE OF DIFFERENT METRICS #\n",
        "    bleu1_scores_plot.append(best_scores['BLEU-1'])\n",
        "    bleu2_scores_plot.append(best_scores['BLEU-2'])\n",
        "    bleu3_scores_plot.append(best_scores['BLEU-3'])\n",
        "    bleu4_scores_plot.append(best_scores['BLEU-4'])\n",
        "    meteor_scores_plot.append(best_scores['METEOR'])\n",
        "    rouge_l_scores_plot.append(best_scores['ROUGE-L'] )\n",
        "    cider_scores_plot.append(best_scores['CIDEr'])\n",
        "    spice_scores_plot.append(best_scores['SPICE'])\n",
        "\n",
        "    print(f'Epoch {epoch } - Evaluation scores:')\n",
        "    print ('For epoch: {}, the train loss is {:.3f}, validation loss is {:.3f}'.format(epoch ,avg_train_loss, avg_val_loss))\n",
        "    print(f'BLEU-1: {best_scores[\"BLEU-1\"]:.4f} BLEU-2: {best_scores[\"BLEU-2\"]:.4f} BLEU-3: {best_scores[\"BLEU-3\"]:.4f} BLEU-4: {best_scores[\"BLEU-4\"]:.4f} METEOR: {best_scores[\"METEOR\"]:.4f}  ROUGE-L: {best_scores[\"ROUGE-L\"]:.4f} CIDEr: {best_scores[\"CIDEr\"]:.4f} SPICE: {best_scores[\"SPICE\"]:.4f}')\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        # print('Validation loss has improved from {:.3f} to {:.3f}'.format(best_val_loss, avg_val_loss))\n",
        "        best_val_loss = avg_val_loss\n",
        "        wait = 0\n",
        "        ckpt_manager.save()\n",
        "    else:\n",
        "        wait += 1\n",
        "        # print(f'No improvement in validation loss. Current patience: {wait}/{patience}')\n",
        "        if wait >= patience:\n",
        "            print('===================')\n",
        "            early_stop = True\n",
        "            break\n",
        "\n",
        "    if early_stop:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING AND VALIDATION LOSS PLOT\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(12, 8))\n",
        "plt.plot(train_loss_plot, color='blue', label = 'Training Loss')\n",
        "plt.plot(val_loss_plot, color='red', label ='Validation Loss')\n",
        "plt.xlabel('Epochs', fontsize = 15, color = 'navy')\n",
        "plt.ylabel('Loss', fontsize = 15, color = 'navy')\n",
        "plt.title('Loss Plot', fontsize = 20, color = 'navy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yC3LCQaoDuE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation score plotting#\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(12, 8))\n",
        "plt.plot(bleu1_scores_plot, color='#fa1007', label = 'BLEU-1')\n",
        "plt.plot(bleu4_scores_plot, color='#595343', label = 'BLEU-4')\n",
        "plt.plot(rouge_l_scores_plot, color='black', label = 'ROUGE-L')\n",
        "plt.plot(meteor_scores_plot, color='#0070fa', label = 'METEOR')\n",
        "plt.plot(cider_scores_plot, color='#a3337c', label = 'CIDEr')\n",
        "plt.plot(spice_scores_plot, color='#94c28f', label = 'SPICE')\n",
        "plt.xlabel('Epochs', fontsize = 15, color = 'navy')\n",
        "plt.ylabel('Score', fontsize = 15, color = 'navy')\n",
        "plt.title('Evaluatation Metrics', fontsize = 20, color = 'navy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a9GVD1bABfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIDER Plotting\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(12, 8))\n",
        "plt.plot(cider_scores_plot, color='#a3337c', label = 'CIDEr')\n",
        "plt.xlabel('Epochs', fontsize = 15, color = 'navy')\n",
        "plt.ylabel('CIDEr Optimization', fontsize = 15, color = 'navy')\n",
        "plt.title(' Evaluatation Metric', fontsize = 20, color = 'navy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MoNsVAM-BfdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "def evaluate(image):\n",
        "    height, width = 16, 16  # 16x16 spatial grid\n",
        "\n",
        "    attention_plot = np.zeros((max_length, height, width))  # Initialize attention plot (word_length, height, width)\n",
        "\n",
        "    hidden = decoder.init_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_images(image)[0], 0)  # Preprocess image\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        # Remove the class token attention weight and reshape the remaining 256 to (16, 16)\n",
        "        attention_weights = tf.reshape(attention_weights[:, 1:], (height, width))\n",
        "        attention_plot[i] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            attention_plot = attention_plot[:len(result), :]  # keep attention up to the result length\n",
        "            return result, attention_plot, predictions\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]  # Final attention plot\n",
        "    return result, attention_plot, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def filt_text(text):\n",
        "    filt=['<start>','<unk>','<end>']\n",
        "    temp= text.split()\n",
        "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
        "    text=' '.join(temp)\n",
        "    return text\n",
        "\n",
        "image_test = path_val.copy()"
      ],
      "metadata": {
        "id": "5u6joOrTBfz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import urllib.request\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def add_color_box_label(fig, text, color, position, edgecolor='black'):\n",
        "    ax = fig.add_axes(position)\n",
        "    # Add a rectangle with a border\n",
        "    rect = Rectangle((0, 0), 1, 1, facecolor=color, edgecolor=edgecolor, linewidth=1)\n",
        "    ax.add_patch(rect)\n",
        "    ax.axis('off')\n",
        "    # Add the text label next to the colored box\n",
        "    fig.text(position[0] + position[2] + 0.01, position[1] + position[3] / 2, text,\n",
        "             va='center', ha='left', fontsize=9)\n",
        "\n",
        "def show_attention_plot(image_path, attention, caption, save_image=False):\n",
        "    # Load image from URL if needed\n",
        "    if image_path.startswith('http'):\n",
        "        image = np.asarray(bytearray(urllib.request.urlopen(image_path).read()), dtype=\"uint8\")\n",
        "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    else:\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "    num_words = len(caption)\n",
        "    num_cols = 5  # Number of images per row\n",
        "    num_rows = (num_words + num_cols - 1) // num_cols  # Compute number of rows\n",
        "\n",
        "    # Set up plot grid with dynamic rows\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))\n",
        "\n",
        "    # Flatten axes for easier indexing, if there's only one row it might not be a list\n",
        "    if num_rows == 1:\n",
        "        axes = axes.flatten()\n",
        "    else:\n",
        "        axes = axes.ravel()\n",
        "\n",
        "    # Create a variable to store the attention maps for the color bar\n",
        "    all_att_maps = []\n",
        "\n",
        "    for i in range(num_words):\n",
        "        word = caption[i]\n",
        "        att_map = attention[i]\n",
        "\n",
        "        # Resize the attention map to the image size\n",
        "        att_map_resized = cv2.resize(att_map, (image.shape[1], image.shape[0]))\n",
        "\n",
        "        # Normalize attention map between 0 and 1\n",
        "        att_map_resized = att_map_resized / np.max(att_map_resized)\n",
        "\n",
        "        # Store for global color bar scaling\n",
        "        all_att_maps.append(att_map_resized)\n",
        "\n",
        "        # Plot image\n",
        "        axes[i].imshow(image, alpha=0.8)\n",
        "\n",
        "        # Overlay attention map\n",
        "        im = axes[i].imshow(att_map_resized, cmap='hot', alpha=0.5)  # Heatmap overlay\n",
        "        axes[i].contour(att_map_resized, colors='yellow', linewidths=0.3)\n",
        "\n",
        "        axes[i].set_title(f\" {word}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "        if save_image:\n",
        "            # Save each image with heatmap\n",
        "            plt.imsave(f'heatmap_{word}.png', att_map_resized, cmap='jet')\n",
        "\n",
        "    # Hide any extra subplots if num_words is not a multiple of num_cols\n",
        "    for j in range(num_words, num_rows * num_cols):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    # Combine all attention maps for color bar normalization\n",
        "    all_att_maps_combined = np.vstack(all_att_maps)\n",
        "\n",
        "    # Add a single horizontal color bar for the entire plot at the top with increased height\n",
        "    fig.subplots_adjust(top=0.78, bottom=0.1, right=0.95, left=0.05, hspace=0.3)  # Adjusted top for more space\n",
        "    bar_width = 0.5  # Width of the color bar\n",
        "    left_position = (1 - bar_width) / 2  # Center the color bar\n",
        "\n",
        "    cbar_ax = fig.add_axes([left_position, 0.85, bar_width, 0.06])  # Centered color bar\n",
        "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
        "    cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1])  # Ticks at normalized values\n",
        "    cbar.set_ticklabels(['Low', '0.2', '0.4', '0.6', '0.8', 'High'])  # Custom labels\n",
        "    fig.text(0.5, 0.98, 'Attention Focus Weight', ha='center', fontsize=12)\n",
        "    add_color_box_label(fig, ' Focused Area', '#fdf98a', [0.8, 0.84, 0.02, 0.06])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "hDFLriSeCZTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predicted_captiontest(image_name, autoplay=False) :\n",
        "    autoplay = False\n",
        "    print(image_name)\n",
        "    print()\n",
        "\n",
        "    rid = image_test.index(image_name)\n",
        "    # print(rid)\n",
        "\n",
        "    cap_test_data = caption_val.copy()\n",
        "    # rid = 100\n",
        "    # rid = random.sample(range(0, rndm),2)\n",
        "    real_cap_list=[]\n",
        "    pred_cap_list=[]\n",
        "    j=rid\n",
        "    test_image=image_test[j]\n",
        "    #print(len(cap_test_data))\n",
        "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[j] if i not in [0]])\n",
        "    #print(real_caption)\n",
        "    result, attention_plot, pred_test = evaluate(test_image)\n",
        "    # print(attention_plot.shape)\n",
        "\n",
        "    real_caption=filt_text(real_caption)\n",
        "    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
        "    real_appn = []\n",
        "    real_appn.append(real_caption.split())\n",
        "    real_cap_list.append(real_appn)\n",
        "    pred_cap_list.append(pred_caption.split())\n",
        "\n",
        "    #all_eval_method(test_image,real_cap_list, pred_cap_list)\n",
        "\n",
        "    print(\"\\033[1mReal Caption:\\033[0m\", real_caption)\n",
        "    print(\"\\033[1mPrediction Caption:\\033[0m\", pred_caption)\n",
        "   # plot_attention_map(result, attention_plot, test_image)\n",
        "    show_attention_plot(test_image, attention_plot, result)\n",
        "    # show_attention_stac(test_image, attention_plot, result)\n",
        "    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)\n",
        "    speech.save('voice.mp3')\n",
        "    audio_file = 'voice.mp3'\n",
        "\n",
        "    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))\n",
        "\n",
        "    return test_image"
      ],
      "metadata": {
        "id": "JdPmGPa8Cvu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_cap_for_test(image_name, autoplay=False) :\n",
        "    autoplay = False\n",
        "    print(image_name)\n",
        "    print()\n",
        "\n",
        "    pred_cap_list=[]\n",
        "    # j=rid\n",
        "    test_image=image_name\n",
        "    result, attention_plot, pred_test = evaluate(test_image)\n",
        "    # print(attention_plot.shape)\n",
        "\n",
        "    # real_caption=filt_text(real_caption)\n",
        "    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
        "    # real_appn = []\n",
        "    # real_appn.append(real_caption.split())\n",
        "    # real_cap_list.append(real_appn)\n",
        "    pred_cap_list.append(pred_caption.split())\n",
        "\n",
        "    #all_eval_method(test_image,real_cap_list, pred_cap_list)\n",
        "\n",
        "    # print(\"\\033[1mReal Caption:\\033[0m\", real_caption)\n",
        "    print(\"\\033[1mPrediction Caption:\\033[0m\", pred_caption)\n",
        "   # plot_attention_map(result, attention_plot, test_image)\n",
        "    show_attention_plot(test_image, attention_plot, result)\n",
        "    # show_attention_stac(test_image, attention_plot, result)\n",
        "    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)\n",
        "    speech.save('voice.mp3')\n",
        "    audio_file = 'voice.mp3'\n",
        "\n",
        "    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))\n",
        "\n",
        "    return test_image"
      ],
      "metadata": {
        "id": "qhdHCGW5C6oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GET CAPTION OF AN IMAGE#\n",
        "image_name =  \"PATH OF AN IMAGE TO GET CAPTION AND ATTENTION PLOT FOR IT\"\n",
        "test_image = predicted_captiontest(image_name, True)\n",
        "Image.open(test_image)"
      ],
      "metadata": {
        "id": "DW29sIvxDJsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GET CAPTION OF AN IMAGE#\n",
        "image_name =  \"PATH OF ANY IMAGE TO GET CAPTION AND ATTENTION PLOT FOR IT\"\n",
        "test_image = pred_cap_for_test(image_name, True)\n",
        "Image.open(test_image)"
      ],
      "metadata": {
        "id": "BydvuZr-Golq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}